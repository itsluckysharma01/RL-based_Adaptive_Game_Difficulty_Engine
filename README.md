<div align="center">

# ğŸ® RL-based Adaptive Game Difficulty Engine

### _Intelligent difficulty adjustment using Reinforcement Learning_

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Pygame](https://img.shields.io/badge/Pygame-2.0+-orange.svg)](https://www.pygame.org)

[Features](#-features) â€¢ [Demo](#-demo) â€¢ [Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Demo](#-demo)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Usage](#-usage)
  - [Training](#training)
  - [Evaluation](#evaluation)
  - [Configuration](#configuration)
- [Project Structure](#-project-structure)
- [Algorithms](#-algorithms)
- [Results](#-results)
- [Contributing](#-contributing)
- [License](#-license)
- [Contact](#-contact)

---

## ğŸŒŸ Overview

Traditional game difficulty settings are **static** and frustrating:

- âŒ Too easy = boring for skilled players
- âŒ Too hard = frustrating for beginners
- âŒ One-size-fits-all approach

This project implements a **Reinforcement Learning (RL) powered adaptive difficulty engine** that:

- âœ… Learns optimal difficulty adjustments from player performance
- âœ… Keeps players in the "**flow state**" - engaged but not overwhelmed
- âœ… Dynamically adapts in real-time using DQN and PPO algorithms

> **Demo Game:** Classic Snake with adaptive speed, obstacles, and food spawn rates

---

## âœ¨ Features

<details open>
<summary><b>ğŸ¤– Reinforcement Learning Algorithms</b></summary>

- **DQN (Deep Q-Network)** - Value-based learning with experience replay
- **PPO (Proximal Policy Optimization)** - Policy gradient method with clipping
- Epsilon-greedy exploration with decay
- Target network stabilization

</details>

<details open>
<summary><b>ğŸ¯ Dynamic Difficulty Adjustment</b></summary>

- Real-time game parameter modification
- Speed adjustment (game pace)
- Obstacle density control
- Food spawn rate tuning
- Multi-level difficulty scaling (1-5)

</details>

<details open>
<summary><b>ğŸ“Š Performance Tracking</b></summary>

- Score and survival time metrics
- Win/loss ratio analysis
- Player engagement indicators
- Episode-based statistics
- Real-time visualization

</details>

<details open>
<summary><b>ğŸ”§ Modular & Configurable</b></summary>

- YAML-based hyperparameter configuration
- Pluggable game interface
- Customizable reward functions
- Easy integration with other games
- Save/load trained models

</details>

---

## ğŸ¬ Demo

### Gameplay with Adaptive Difficulty

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Score: 15        Difficulty: â­â­â­    â”‚
â”‚  ğŸ Snake speeds up as you improve!    â”‚
â”‚  ğŸ“Š AI adjusts in real-time             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> _Place your gameplay GIF here: `docs/results_screenshots/gameplay.gif`_

### Training Progress

> _Training curves showing agent learning: `plots/training_curves.png`_

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   GAME ENVIRONMENT                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Snake    â”‚â†’ â”‚   Metrics    â”‚â†’ â”‚  Difficulty â”‚ â”‚
â”‚  â”‚   Game     â”‚  â”‚   Tracker    â”‚  â”‚   Manager   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ State (score, time, difficulty)
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               RL AGENT (DQN / PPO)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Neural Network: State â†’ Action                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”             â”‚ â”‚
â”‚  â”‚  â”‚Input â”‚ â†’ â”‚Hidden â”‚ â†’ â”‚Outputâ”‚             â”‚ â”‚
â”‚  â”‚  â”‚ (4)  â”‚   â”‚(128) â”‚   â”‚ (3)  â”‚             â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ Action (easier/harder/maintain)
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DIFFICULTY ADJUSTMENT                    â”‚
â”‚    Speed â†‘/â†“   Obstacles â†‘/â†“   Spawns â†‘/â†“          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

| Component              | Description                                           |
| ---------------------- | ----------------------------------------------------- |
| **Game Interface**     | Snake game built with Pygame                          |
| **Metrics Tracker**    | Monitors player performance (score, survival, deaths) |
| **Difficulty Manager** | Executes difficulty parameter changes                 |
| **RL Agent**           | Makes intelligent adjustment decisions                |
| **Reward Function**    | Evaluates quality of difficulty adjustments           |

---

## ğŸ“¦ Installation

<details open>
<summary><b>ğŸ Prerequisites</b></summary>

- Python 3.8 or higher
- pip package manager
- Git (optional)

</details>

### Clone the Repository

```bash
git clone https://github.com/yourusername/RL-based_Adaptive_Game_Difficulty_Engine.git
cd RL-based_Adaptive_Game_Difficulty_Engine
```

### Install Dependencies

```bash
pip install -r requirements.txt
```

**Required packages:**

```
pygame       # Game development
numpy        # Numerical computations
torch        # Deep learning framework
matplotlib   # Visualization
pyyaml       # Configuration files
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Test Pre-trained Models

```bash
# Evaluate DQN agent
python evaluate.py dqn models/dqn_final.pth

# Evaluate PPO agent
python evaluate.py ppo models/ppo_final.pth
```

### 2ï¸âƒ£ Train Your Own Agent

```bash
# Train DQN (1000 episodes)
python train.py dqn

# Train PPO (1000 episodes)
python train.py ppo
```

### 3ï¸âƒ£ Play Manually

```bash
# Play Snake without AI
python game/snake.py
```

**Controls:**

- â¬†ï¸ â¬‡ï¸ â¬…ï¸ â¡ï¸ Arrow keys to move
- `R` - Restart game
- `ESC` - Exit

---

## ğŸ“– Usage

### Training

<details>
<summary><b>Train DQN Agent</b></summary>

```bash
python train.py dqn
```

**What happens:**

1. Initializes Snake game environment
2. Creates DQN agent with replay buffer
3. Trains for 1000 episodes (configurable)
4. Saves checkpoints every 50 episodes
5. Generates training plots in `plots/`

**Output:**

```
Episode 50/1000: Score=15.2, Reward=145.3, Epsilon=0.81
Episode 100/1000: Score=18.5, Reward=167.8, Epsilon=0.66
...
Training complete! Model saved to models/dqn_final.pth
```

</details>

<details>
<summary><b>Train PPO Agent</b></summary>

```bash
python train.py ppo
```

**PPO-specific features:**

- Actor-Critic architecture
- Policy and value function training
- Clipped surrogate objective
- Multiple epochs per batch

</details>

### Evaluation

```bash
# Run 5 evaluation episodes
python evaluate.py dqn models/dqn_final.pth

# Custom episodes
python evaluate.py ppo models/ppo_final.pth --episodes 10
```

**Evaluation Metrics:**

- Average score Â± std
- Best score achieved
- Average survival time
- Difficulty adaptation patterns

### Configuration

Edit [`config/hyperparameters.yaml`](config/hyperparameters.yaml):

```yaml
# DQN Hyperparameters
dqn:
  learning_rate: 0.001
  gamma: 0.99 # Discount factor
  epsilon_start: 1.0 # Exploration rate
  epsilon_min: 0.01
  epsilon_decay: 0.995
  batch_size: 64
  memory_size: 10000
  hidden_size: 128

# Training Configuration
training:
  episodes: 1000 # Total training episodes
  max_steps: 500 # Steps per episode
  save_frequency: 50 # Checkpoint frequency
  render: false # Show game window

# Environment Configuration
environment:
  state_size: 4 # [score, time, deaths, difficulty]
  action_size: 3 # [harder, easier, maintain]
```

---

## ğŸ“ Project Structure

```
RL-based_Adaptive_Game_Difficulty_Engine/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                      # You are here!
â”œâ”€â”€ ğŸ“„ requirements.txt               # Dependencies
â”œâ”€â”€ ğŸ“„ train.py                       # Training script
â”œâ”€â”€ ğŸ“„ evaluate.py                    # Evaluation script
â”‚
â”œâ”€â”€ ğŸ“ agent/                         # RL Algorithms
â”‚   â”œâ”€â”€ dqn.py                       # Deep Q-Network
â”‚   â”œâ”€â”€ ppo.py                       # Proximal Policy Optimization
â”‚   â””â”€â”€ replay_buffer.py             # Experience replay
â”‚
â”œâ”€â”€ ğŸ“ game/                          # Game Environment
â”‚   â”œâ”€â”€ snake.py                     # Snake game implementation
â”‚   â”œâ”€â”€ difficulty_manager.py       # Difficulty adjustment logic
â”‚   â””â”€â”€ metrics.py                   # Performance tracking
â”‚
â”œâ”€â”€ ğŸ“ config/                        # Configuration
â”‚   â””â”€â”€ hyperparameters.yaml        # Training/model parameters
â”‚
â”œâ”€â”€ ğŸ“ models/                        # Saved Models
â”‚   â”œâ”€â”€ dqn_final.pth               # Trained DQN
â”‚   â””â”€â”€ ppo_final.pth               # Trained PPO
â”‚
â”œâ”€â”€ ğŸ“ plots/                         # Training Visualizations
â”‚   â”œâ”€â”€ dqn_training_curve.png
â”‚   â””â”€â”€ ppo_training_curve.png
â”‚
â”œâ”€â”€ ğŸ“ notebook/                      # Analysis
â”‚   â””â”€â”€ Snake_Adaptive_RL.ipynb     # Jupyter notebook
â”‚
â””â”€â”€ ğŸ“ docs/                          # Documentation
    â””â”€â”€ results_screenshots/         # Screenshots & GIFs
```

---

## ğŸ§  Algorithms

### Deep Q-Network (DQN)

**Key Features:**

- Experience replay buffer (reduces correlation)
- Target network (stabilizes training)
- Epsilon-greedy exploration

**State Space:**

```python
state = [
    score,              # Current game score
    survival_time,      # Time alive in seconds
    deaths,             # Death count this episode
    difficulty_level    # Current difficulty (1-5)
]
```

**Action Space:**

```python
actions = {
    0: "Make game harder",
    1: "Make game easier",
    2: "Maintain difficulty"
}
```

**Reward Function:**

```python
reward = score_increase * 10        # Reward for scoring
       + survival_time * 0.1        # Bonus for staying alive
       - 50 (if game_over)          # Penalty for dying
```

### Proximal Policy Optimization (PPO)

**Advantages:**

- More stable than vanilla policy gradients
- Better sample efficiency
- Suitable for continuous control

**Architecture:**

- **Actor:** Outputs action probabilities
- **Critic:** Estimates state value
- **Clipped Objective:** Prevents large policy updates

---

## ğŸ“Š Results

### Training Performance

| Metric              | DQN           | PPO           |
| ------------------- | ------------- | ------------- |
| **Convergence**     | ~300 episodes | ~250 episodes |
| **Final Avg Score** | 18.5 Â± 3.2    | 21.3 Â± 2.8    |
| **Best Score**      | 45            | 52            |
| **Training Time**   | ~45 min       | ~60 min       |

### Difficulty Adaptation Patterns

> ğŸ“ˆ _Add your training curves from `plots/` directory_

**Key Findings:**

- âœ… Agents learn to reduce difficulty after player deaths
- âœ… Difficulty increases when player performs consistently well
- âœ… Maintains "flow state" better than static difficulty
- âœ… PPO shows more stable difficulty adjustments

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

<details>
<summary><b>ğŸ› Report Bugs</b></summary>

Open an issue with:

- Description of the bug
- Steps to reproduce
- Expected vs actual behavior
- Screenshots (if applicable)

</details>

<details>
<summary><b>ğŸ’¡ Suggest Features</b></summary>

We're looking for:

- New RL algorithms (A3C, SAC, TD3)
- Additional games (Pong, Flappy Bird, etc.)
- Better reward function designs
- Hyperparameter tuning strategies

</details>

<details>
<summary><b>ğŸ”§ Submit Pull Requests</b></summary>

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

</details>

### Development Setup

```bash
# Clone your fork
git clone https://github.com/yourusername/RL-based_Adaptive_Game_Difficulty_Engine.git

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install in development mode
pip install -r requirements.txt
```

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License - Feel free to use this project for:
âœ… Personal projects
âœ… Commercial applications
âœ… Research and education
âœ… Modification and distribution
```

---

## ğŸ“§ Contact

**Project Maintainer:** Lucky Sharma

- ğŸ“§ Email: itsluckysharma001@gmail.com
- ğŸ™ GitHub: [@itsluckysharma01](https://github.com/itsluckysharma01)
- ğŸ’¼ LinkedIn: [Lucky Sharma](https://www.linkedin.com/in/lucky-sharma918894599977/)

---

## ğŸ™ Acknowledgments

- Inspired by research on **Flow Theory** in game design
- Built with [PyTorch](https://pytorch.org/) and [Pygame](https://www.pygame.org/)
- DQN algorithm based on [Mnih et al., 2015](https://www.nature.com/articles/nature14236)
- PPO algorithm from [Schulman et al., 2017](https://arxiv.org/abs/1707.06347)

---

## ğŸ“š Further Reading

- [Adaptive Game AI with Reinforcement Learning](https://ieeexplore.ieee.org/)
- [Dynamic Difficulty Adjustment in Games](https://www.gamasutra.com/)
- [Flow Theory and Player Experience](<https://en.wikipedia.org/wiki/Flow_(psychology)>)

---

<div align="center">

### â­ Star this project if you find it useful!

**Made with â¤ï¸ and ğŸ¤– by [Your Name]**

[â¬† Back to Top](#-rl-based-adaptive-game-difficulty-engine)

</div>
